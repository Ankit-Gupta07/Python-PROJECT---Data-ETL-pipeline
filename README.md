# Python-PROJECT---Data-ETL-pipeline
A jupyter notebook with simple explanations for all of the code. The code EXTRACTS raw data from various sources, TRANSFORMS it with every necessary step, and finally LOADs it to various destinations. It displays the whole process (with all the steps) of a critical skill - "data engineering".

# ETL: Extract, Transform, Load with Python
<br>"Welcome to the repository ! This project showcases the entire process of ""data ETL"". Its main goal  is to illustrate the ETL process through a Python-based implementation. The project is structured into three main sections: Extract, Transform, and Load. Each section performs its designated tasks.

i) **EXTRACT** - We first extract data from these sources - CSVs, APIs, JSONs, XMLs, Databases (SQLite & PostgreSQL) & store it in a pandas dataframe using Python libraries like pandas, requests, BeautifulSoup, sqlite3, and pyscopg2.

ii) **TRANSFORM** - Here we make the data more suitable for analysis by applying these steps on data - combine, clean, type conversion, handle missing & duplicate values, fitting a linear regression model, finding & removing outliers, feature scaling and engineering using Python libraries like numpy, pandas, pycountry, sklearn, and matplotlib.  

iii) **LOAD** - Lastly we store the transformed data into these destinations - CSVs, JSONs and Databases (SQLite & PostgreSQL) using Python libraries like pandas, sqlite, and pyscopg2

The end goal of the notebook is to combine these data sets together so that we can run a machine learning model predicting World Bank Project total costs or the GDP of a country as per their population or anything you want."
